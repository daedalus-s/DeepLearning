{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill your name and NetID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Aaron Lozhkin\"\n",
    "NET_ID = \"al1336\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Assignment 3: RNN, LSTM, Attention, and Transformers\n",
    "\n",
    "In this assignment, you will implement neural machine translation (NMT) models using:\n",
    "\n",
    "1. RNNs\n",
    "2. LSTMs and LSTMs with attention\n",
    "3. Transformers.\n",
    "\n",
    "As in the previous assignments, you will see code blocks that look like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "x = 3.7\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "Also, please remember:\n",
    "- Do not write or modify any code outside of code blocks\n",
    "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
    "- Run all cells before submitting. You will only get credit for code that has been run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First let's import some libraries that will be useful in this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')\n",
    "  device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_if_not_exist(file_name):\n",
    "  \n",
    "  if not os.path.exists(file_name):\n",
    "    import urllib.request\n",
    "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "\n",
    "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
    "    \n",
    "  return file_name\n",
    "\n",
    "def read_raw(file_name):\n",
    "  file_name = download_if_not_exist(file_name)\n",
    "  \n",
    "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
    "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
    "  return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Wow!\tÇa alors !\n",
      "Fire!\tAu feu !\n",
      "Help!\tÀ l'aide !\n",
      "Jump.\tSaute.\n",
      "Stop!\tÇa suffit !\n",
      "Stop!\tStop !\n",
      "Stop!\tArrête-toi !\n",
      "Wait!\tAttends !\n",
      "Wait!\tAttendez !\n",
      "I see.\tJe comprends.\n"
     ]
    }
   ],
   "source": [
    "raw_text = read_raw('./data.zip')\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "  out = ''\n",
    "  for i, char in enumerate(text.lower()):\n",
    "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
    "      out += ' '\n",
    "    out += char\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further split the source-target pairs into a source list and a target list. We use word-level tokenization here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_target(text, max_len):\n",
    "  source, target = [], []\n",
    "  for i, line in enumerate(text.split('\\n')):\n",
    "    if i > 5000: # we only use 5000 pairs of translation\n",
    "      break\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) == 2:\n",
    "      src_tokens = parts[0].split(' ')\n",
    "      tgt_tokens = parts[1].split(' ')\n",
    "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
    "        source.append(src_tokens)\n",
    "        target.append(tgt_tokens)\n",
    "  return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(raw_text, max_len=10000):\n",
    "  text = preprocess_raw(raw_text)\n",
    "  source, target = split_source_target(text, max_len)\n",
    "  return source, target\n",
    "\n",
    "source, target = prepare_data(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def len_dis(text):\n",
    "  lens = [len(line) for line in text]\n",
    "  len_counter = collections.Counter(lens)\n",
    "\n",
    "  lens = np.array(list(len_counter.keys()))\n",
    "  sort_idx = np.argsort(lens)\n",
    "  lens_sort = lens[sort_idx]\n",
    "  len_counts = np.array(list(len_counter.values()))\n",
    "  len_counts_sort = len_counts[sort_idx]\n",
    "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
    "  return p, lens_sort\n",
    "  \n",
    "src_p, src_lens_sort = len_dis(source)\n",
    "tgt_p, tgt_lens_sort = len_dis(target)\n",
    "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
    "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
    "plt.title('Cumulative Distribution of Sentence Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param\n",
    "MAX_LEN = 8\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary\n",
    "\n",
    "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
    "- Dictionary word2index: Convert word string into index: \n",
    "- Dictionary index2word: Convert index into word string\n",
    "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
    "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary \n",
    "    \n",
    "There are 4 special tokens added in the vocabulary:\n",
    "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
    "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
    "- 'eos': end of sentence. This indicates the end of a sentence\n",
    "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "  def __init__(self, name, tokens, min_freq):\n",
    "    self.name = name\n",
    "    self.index2word = {\n",
    "      0: 'pad',\n",
    "      1: 'bos',\n",
    "      2: 'eos',\n",
    "      3: 'unk'\n",
    "    }\n",
    "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
    "    self.num_word = 4\n",
    "    token_freq = collections.Counter(tokens)\n",
    "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
    "    self._build_vocab(tokens)\n",
    "    \n",
    "  def _build_vocab(self, tokens):\n",
    "    for token in tokens:\n",
    "      if token not in self.word2index:\n",
    "        self.word2index[token] = self.num_word\n",
    "        self.index2word[self.num_word] = token\n",
    "        self.num_word += 1\n",
    "        \n",
    "  def __getitem__(self, tokens):\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "      return self.word2index.get(tokens, self.word2index['unk'])\n",
    "    else:\n",
    "      return [self.__getitem__(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset\n",
    "\n",
    "The dataset pipeline involves the following steps:\n",
    "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
    "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
    "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
    "- We use the Pytorch *DataLoader* API to build the dataset generator.\n",
    "\n",
    "For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
      "Vocabulary size of source language: 433\n",
      "Vocabulary size of target language: 420\n",
      "Total number of sentence pairs: 4990\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(name, tokens, min_freq):\n",
    "  tokens = [token for line in tokens for token in line]\n",
    "  return Vocab(name, tokens, min_freq)\n",
    "\n",
    "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
    "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
    "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
    "  return vocab_src, vocab_tgt\n",
    "\n",
    "def pad(line, padding_token):\n",
    "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
    "\n",
    "def build_tensor(text, lang, is_source):\n",
    "  lines = [lang[line] for line in text]\n",
    "  if not is_source:\n",
    "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
    "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
    "  valid_len = (array != lang['pad']).sum(1)\n",
    "  return array, valid_len\n",
    "\n",
    "def load_data_nmt(batch_size=2):\n",
    "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
    "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
    "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
    "  train_data = torch.utils.data.TensorDataset(\n",
    "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "  print(train_data[0])\n",
    "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "  return lang_eng, lang_fra, train_iter\n",
    "\n",
    "\n",
    "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
    "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
    "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
    "print('Total number of sentence pairs: {}'.format(len(source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence with RNN (baseline)\n",
    "\n",
    "In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
    "\n",
    "Implementation Details:\n",
    "\n",
    "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
    "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
    "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
    "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
    "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n",
    "</div>\n",
    "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
    "    \"\"\"\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
    "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, sources, valid_len):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
    "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
    "    \"\"\"\n",
    "    word_embedded = self.embedding(sources)\n",
    "    N = word_embedded.shape[0]\n",
    "    \n",
    "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
    "    \n",
    "    o, h = self.enc(word_embedded, h)\n",
    "    \n",
    "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "    super(Decoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
    "    \"\"\"\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, h, target):\n",
    "    word_embedded = self.embedding(target)\n",
    "    N, T = word_embedded.shape[:2]\n",
    "    \n",
    "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
    "    pred = self.output_emb(o)\n",
    "    return pred, h\n",
    "\n",
    "class NMTRNN(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
    "    super(NMTRNN, self).__init__()\n",
    "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
    "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
    "    \n",
    "  def forward(self, src, src_len, tgt, tgt_len):\n",
    "    h = self.enc(src, src_len)\n",
    "    T = tgt.shape[1]\n",
    "    \n",
    "    pred, _ = self.dec(h, tgt)\n",
    "    loss = 0\n",
    "\n",
    "    # print(\"pred Size: \", pred.size())\n",
    "    # print(np.shape(target))\n",
    "    # print(T-1)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "      # print(tgt[:, t+1])\n",
    "      # target sequence should shift by one time-step, because we are predicting the next word\n",
    "      # notice the `ignore_index` parameter is set to 0, which is for the `pad` token\n",
    "      # print(\"Size before loss (pred): \", pred.size())\n",
    "      loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
    "\n",
    "    return loss, pred.argmax(dim=-1)\n",
    "\n",
    "  def predict(self, src, src_len, tgt, tgt_len):\n",
    "      \"\"\"\n",
    "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
    "      token from the previous time step.\n",
    "      \"\"\"\n",
    "      h = self.enc(src, src_len)\n",
    "\n",
    "      inputs = tgt[:, :1]\n",
    "      preds = []\n",
    "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
    "        pred, h = self.dec(h, inputs)\n",
    "        preds.append(pred)\n",
    "        inputs = pred.argmax(dim=-1)\n",
    "        \n",
    "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
    "      return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 7800\tLoss:\t41.284470\n",
      "pred:\t tensor([146,  65, 349,  53,   3, 330,  52, 330, 330, 330])\n",
      "\n",
      "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 156 / 7800\tLoss:\t11.373284\n",
      "pred:\t tensor([14,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([15, 89,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 312 / 7800\tLoss:\t10.170557\n",
      "pred:\t tensor([ 14, 164,   3,  11,  11,   2,  11,   3,   3,   3])\n",
      "\n",
      "tgt:\t tensor([ 48,  49, 113, 282,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 468 / 7800\tLoss:\t9.460774\n",
      "pred:\t tensor([ 3,  3, 11,  3, 11,  2, 11,  3,  3, 11])\n",
      "\n",
      "tgt:\t tensor([36,  3, 37,  3,  5,  2,  0,  0,  0])\n",
      "\n",
      "iter 624 / 7800\tLoss:\t7.220533\n",
      "pred:\t tensor([36,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([36,  9, 88, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 780 / 7800\tLoss:\t7.546391\n",
      "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11, 11, 11])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 936 / 7800\tLoss:\t7.726661\n",
      "pred:\t tensor([ 14,  28,   3, 227,  37,   3,  11,   2,  11,   3])\n",
      "\n",
      "tgt:\t tensor([ 14,  28,  35, 227,  37,   3,  11,   2,   0])\n",
      "\n",
      "iter 1092 / 7800\tLoss:\t6.480487\n",
      "pred:\t tensor([171, 342,   3,  11,   2,  11,   3,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 1248 / 7800\tLoss:\t7.046074\n",
      "pred:\t tensor([ 14, 116,  72,  11,   2,  11,   3,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([ 14, 197,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 1404 / 7800\tLoss:\t5.653791\n",
      "pred:\t tensor([ 14, 171,   3,  11,   2,  11,   3,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([14, 70,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 1560 / 7800\tLoss:\t6.483207\n",
      "pred:\t tensor([168, 203,  78,   3,  41,  24,   2,   5,   3,   3])\n",
      "\n",
      "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
      "\n",
      "iter 1716 / 7800\tLoss:\t6.235534\n",
      "pred:\t tensor([ 14, 171,   3, 205,   5,   2,  11,   3,   3,   3])\n",
      "\n",
      "tgt:\t tensor([ 14,  70,  17, 205,   5,   2,   0,   0,   0])\n",
      "\n",
      "iter 1872 / 7800\tLoss:\t4.705895\n",
      "pred:\t tensor([ 3,  5,  2, 11,  3, 11, 11, 11, 11, 11])\n",
      "\n",
      "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 2028 / 7800\tLoss:\t5.638156\n",
      "pred:\t tensor([ 14, 171,   3,  75,   3,  11,   2,  11,   3,  11])\n",
      "\n",
      "tgt:\t tensor([ 14, 171, 213,  75,   3,  11,   2,   0,   0])\n",
      "\n",
      "iter 2184 / 7800\tLoss:\t5.068721\n",
      "pred:\t tensor([ 93, 203,  14, 171,   3,  24,   2,  11,   3,  11])\n",
      "\n",
      "tgt:\t tensor([176, 203,  14, 116,   3,  24,   2,   0,   0])\n",
      "\n",
      "iter 2340 / 7800\tLoss:\t4.763160\n",
      "pred:\t tensor([14, 28, 11,  2, 11,  3, 11, 11, 11, 11])\n",
      "\n",
      "tgt:\t tensor([103, 385,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 2496 / 7800\tLoss:\t5.181518\n",
      "pred:\t tensor([15,  3, 72,  3, 11,  2, 11,  3, 11, 11])\n",
      "\n",
      "tgt:\t tensor([ 15, 291,   9,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2652 / 7800\tLoss:\t5.213562\n",
      "pred:\t tensor([ 14, 116, 179, 259,  11,   2,  11,   3,  11,   3])\n",
      "\n",
      "tgt:\t tensor([ 14, 116, 179, 305,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2808 / 7800\tLoss:\t3.892561\n",
      "pred:\t tensor([ 36, 316,  41, 139,  11,   2,  11,   3,   3,   3])\n",
      "\n",
      "tgt:\t tensor([315, 316,  41, 139,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2964 / 7800\tLoss:\t3.334380\n",
      "pred:\t tensor([ 3, 11,  2, 11,  3,  3,  3,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 3120 / 7800\tLoss:\t4.012745\n",
      "pred:\t tensor([198, 265,  11,   2,  11,   3,   3,   3,   3,   3])\n",
      "\n",
      "tgt:\t tensor([  3, 265,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 3276 / 7800\tLoss:\t3.445902\n",
      "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11,  3, 11])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 3432 / 7800\tLoss:\t3.287418\n",
      "pred:\t tensor([ 3,  5,  3, 11,  2, 11,  3, 11, 11, 11])\n",
      "\n",
      "tgt:\t tensor([  3,  72, 248,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 3588 / 7800\tLoss:\t3.999411\n",
      "pred:\t tensor([176,   3, 302,  24,   2,  24, 226,   5,   5,   5])\n",
      "\n",
      "tgt:\t tensor([  3,  37, 355,  24,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 3744 / 7800\tLoss:\t3.889436\n",
      "pred:\t tensor([ 14, 116,  74,   3,  11,   2,  11,   3,  11,   3])\n",
      "\n",
      "tgt:\t tensor([ 14, 289,  74,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 3900 / 7800\tLoss:\t3.209066\n",
      "pred:\t tensor([176,  72, 171,  24,   2,  24,   3,   3,   3,  11])\n",
      "\n",
      "tgt:\t tensor([176,   9, 237,  24,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 4056 / 7800\tLoss:\t3.960310\n",
      "pred:\t tensor([ 38, 116, 203,   3,  11,   2,  11,   3,  11,   3])\n",
      "\n",
      "tgt:\t tensor([ 38, 419, 203,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 4212 / 7800\tLoss:\t3.649015\n",
      "pred:\t tensor([ 90, 176,  24,   2,  24, 120,   5,   5,  11,  11])\n",
      "\n",
      "tgt:\t tensor([ 90, 176,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 4368 / 7800\tLoss:\t3.168023\n",
      "pred:\t tensor([  3, 207,   3,  11,   2,  11,   3,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([  3, 207,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 4524 / 7800\tLoss:\t2.818379\n",
      "pred:\t tensor([  3, 354,   5,   2,  11,   3,   3,   3,  11,  11])\n",
      "\n",
      "tgt:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 4680 / 7800\tLoss:\t2.571377\n",
      "pred:\t tensor([  3, 314,  11,   2,  11,   3,   3,   3,  11,   3])\n",
      "\n",
      "tgt:\t tensor([  3, 314,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 4836 / 7800\tLoss:\t2.608840\n",
      "pred:\t tensor([ 38,  40,  21, 390,  11,   2,  11,   3,   3,   3])\n",
      "\n",
      "tgt:\t tensor([ 38,  40,  21, 390,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 4992 / 7800\tLoss:\t3.738841\n",
      "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 5148 / 7800\tLoss:\t3.358139\n",
      "pred:\t tensor([267,   3,  24,   2,  24, 120,   5,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([267, 234,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 5304 / 7800\tLoss:\t3.010740\n",
      "pred:\t tensor([52,  3, 11,  2, 11,  3, 11,  3, 11, 11])\n",
      "\n",
      "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 5460 / 7800\tLoss:\t2.315676\n",
      "pred:\t tensor([38,  3,  3,  3, 11,  2, 11,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([ 38,  78, 214,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 5616 / 7800\tLoss:\t2.540809\n",
      "pred:\t tensor([48,  3, 11,  3, 11,  2, 11,  3, 11,  3])\n",
      "\n",
      "tgt:\t tensor([48,  3, 37, 10, 11,  2,  0,  0,  0])\n",
      "\n",
      "iter 5772 / 7800\tLoss:\t1.929565\n",
      "pred:\t tensor([ 3, 11, 11,  2, 11,  3, 11, 11, 11,  3])\n",
      "\n",
      "tgt:\t tensor([  3, 161,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 5928 / 7800\tLoss:\t2.494524\n",
      "pred:\t tensor([ 3,  5,  5,  2, 24,  3,  3,  3, 11, 11])\n",
      "\n",
      "tgt:\t tensor([ 3, 19,  5,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 6084 / 7800\tLoss:\t2.593269\n",
      "pred:\t tensor([36,  3, 11,  2, 11,  3, 11, 11, 11,  3])\n",
      "\n",
      "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 6240 / 7800\tLoss:\t1.944715\n",
      "pred:\t tensor([ 3,  5,  2, 11,  3,  3,  3,  3,  3,  3])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 6396 / 7800\tLoss:\t2.694863\n",
      "pred:\t tensor([171, 341, 133,  11,   2,  11,   3,  11,   3,   3])\n",
      "\n",
      "tgt:\t tensor([ 92, 341, 236,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 6552 / 7800\tLoss:\t2.219350\n",
      "pred:\t tensor([267, 117,  24,   2,  24, 120, 364,  11,  11,  11])\n",
      "\n",
      "tgt:\t tensor([267, 118,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 6708 / 7800\tLoss:\t2.703387\n",
      "pred:\t tensor([ 14,  17, 114,  11,   2,  11,   3,  11,   3,  11])\n",
      "\n",
      "tgt:\t tensor([ 14,  17, 114,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 6864 / 7800\tLoss:\t1.569989\n",
      "pred:\t tensor([14, 79, 28, 41,  3, 11,  2, 11,  3, 11])\n",
      "\n",
      "tgt:\t tensor([ 14,  79,  28,  41, 229,  11,   2,   0,   0])\n",
      "\n",
      "iter 7020 / 7800\tLoss:\t2.534427\n",
      "pred:\t tensor([267,   3,  24,   2,  24, 120, 364, 144,  11,  11])\n",
      "\n",
      "tgt:\t tensor([267, 124,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 7176 / 7800\tLoss:\t1.835593\n",
      "pred:\t tensor([ 15, 204, 393,   3,  11,   2,  11,   3,  11,  11])\n",
      "\n",
      "tgt:\t tensor([ 15, 204, 393,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 7332 / 7800\tLoss:\t1.663004\n",
      "pred:\t tensor([52,  3, 11,  2, 11,  3,  3,  3, 11,  3])\n",
      "\n",
      "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 7488 / 7800\tLoss:\t2.070917\n",
      "pred:\t tensor([15,  3, 11,  2, 11,  3,  3,  3, 11, 11])\n",
      "\n",
      "tgt:\t tensor([15, 23, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 7644 / 7800\tLoss:\t1.936537\n",
      "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3, 11])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_rnn(net, train_iter, lr, epochs, device):\n",
    "  # training\n",
    "  net = net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "  loss_list = []\n",
    "  print_interval = len(train_iter)\n",
    "  total_iter = epochs * len(train_iter)\n",
    "  for e in range(epochs):\n",
    "    net.train()\n",
    "    for i, train_data in enumerate(train_iter):\n",
    "      train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "      loss, pred = net(*train_data)\n",
    "\n",
    "      loss_list.append(loss.mean().detach().cpu())\n",
    "      optimizer.zero_grad()\n",
    "      loss.mean().backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      step = i + e * len(train_iter)\n",
    "      if step % print_interval == 0:\n",
    "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "        # print(\"Size before output (pred): \", pred.size())\n",
    "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
    "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
    "  return loss_list\n",
    "\n",
    "seed(1)\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "embedding_dim = 250\n",
    "hidden_size = 128\n",
    "\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
    "\n",
    "rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Loss Curve\n",
    "\n",
    "Plot the loss curve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Curve of Baseline')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the loss curve figure in a file for the report\n",
    "plt.plot(np.arange(len(rnn_loss_list)), rnn_loss_list)\n",
    "plt.title('Loss Curve of Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Accuracy\n",
    "\n",
    "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
      "pred:\t ['unk', '!', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', \"t'en\", 'dois', 'une', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['est-il', 'unk', '?', '?', \"j'adore\", 'la', 'mienne']\n",
      "\n",
      "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', 'suis', 'unk', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['unk', 'à', 'qui', 'que', 'ce', 'soit', '!', '?']\n",
      "\n",
      "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "Prediction Acc.: 0.7168\n"
     ]
    }
   ],
   "source": [
    "def comp_acc(pred, gt, valid_len):\n",
    "  N, T_gt = gt.shape[:2]\n",
    "  _, T_pr = pred.shape[:2]\n",
    "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
    "  len_mask = len_mask < valid_len[:, None]\n",
    "  \n",
    "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "  return pred_acc\n",
    "  \n",
    "def evaluate_rnn(net, train_iter, device):\n",
    "  acc_list = []\n",
    "  for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "    pred = net.predict(*train_data)\n",
    "\n",
    "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
    "    acc_list.append(pred_acc)\n",
    "    if i < 5:# print 5 samples from 5 batches\n",
    "      pred = pred[0].detach().cpu()\n",
    "      pred_seq = []\n",
    "      for t in range(MAX_LEN+1):\n",
    "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "        if pred_wd != 'eos':\n",
    "          pred_seq.append(pred_wd)\n",
    "\n",
    "      print('pred:\\t {}\\n'.format(pred_seq))\n",
    "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
    "\n",
    "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
    "  \n",
    "seed(1)\n",
    "batch_size = 32\n",
    "\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "\n",
    "evaluate_rnn(rnn_net, train_iter, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence with LSTM and Attention\n",
    "\n",
    "Now let's try to improve our model by using an LSTM and the attention mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
    "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
    "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
    "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
    "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
    "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Implement the LSTM class below. In particular,\n",
    "-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n",
    "- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "386a51799571153e76849c4b5ebb7f73",
     "grade": false,
     "grade_id": "cell-e43516618029ca06",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, device):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.device = device\n",
    "    self.params = nn.ParameterList(self.init_params(input_size, hidden_size))\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      input_size: int, feature dimension of input sequence\n",
    "      hidden_size: int, feature dimension of hidden state\n",
    "      device: torch.device()\n",
    "    \"\"\"\n",
    "  \n",
    "  def init_params(self, input_size, hidden_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      input_size: int, feature dimension of input sequence\n",
    "      hidden_size: int, feature dimension of hidden state\n",
    "      \n",
    "    Outputs:\n",
    "      Weights for proposal: W_xc, W_hc, b_c\n",
    "      Weights for input gate: W_xi, W_hi, b_i\n",
    "      Weights for forget gate: W_xf, W_hf, b_f\n",
    "      Weights for output gate: W_xo, W_ho, b_o\n",
    "    \"\"\"\n",
    "    W_xc, W_hc, b_c = None, None, None\n",
    "    W_xi, W_hi, b_i = None, None, None\n",
    "    W_xf, W_hf, b_f = None, None, None\n",
    "    W_xo, W_ho, b_o = None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize the weights and biases. The result will be stored in \n",
    "    # `params` below. Weights should be initialized using `torch.randn` multiplied \n",
    "    # with the scale (0.1). Biases should be initialized to 0.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    W_xc = torch.nn.Parameter(torch.randn(input_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    W_hc = torch.nn.Parameter(torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    b_c = torch.nn.Parameter(torch.zeros(hidden_size, requires_grad=True))\n",
    "\n",
    "    W_xi = torch.nn.Parameter(torch.randn(input_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    W_hi = torch.nn.Parameter(torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    b_i = torch.nn.Parameter(torch.zeros(hidden_size, requires_grad=True))\n",
    "\n",
    "    W_xf = torch.nn.Parameter(torch.randn(input_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    W_hf = torch.nn.Parameter(torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    b_f = torch.nn.Parameter(torch.zeros(hidden_size, requires_grad=True))\n",
    "\n",
    "    W_xo = torch.nn.Parameter(torch.randn(input_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    W_ho = torch.nn.Parameter(torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1)\n",
    "    b_o = torch.nn.Parameter(torch.zeros(hidden_size, requires_grad=True))\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
    "    return params\n",
    "\n",
    "  \n",
    "  def lstm(self, X, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
    "        T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
    "        \n",
    "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n",
    "            (N, hidden_size), is the memory cell of the LSTM.\n",
    "      \n",
    "    Outputs:\n",
    "      o: tensor of size (N, T, hidden_size). Contains the output features (the hidden state H_t) for each t.\n",
    "      state: the same as input state. Contains the hidden state H_T and cell state C_T for the last timestep T.\n",
    "    \"\"\"\n",
    "\n",
    "    src, src_len = X\n",
    "    h, c = state\n",
    "\n",
    "    # make sure always has a T dim\n",
    "    if len(src.shape) == 2:\n",
    "      src = src.unsqueeze(1)\n",
    "\n",
    "    N, T, D_in = src.shape\n",
    "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
    "    o = []\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the forward pass of the LSTM.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "\n",
    "    o = torch.zeros(N, T, h.size(1))\n",
    "\n",
    "    for t in range(T):\n",
    "            x_t = src[:,t,:]\n",
    "\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ W_xi + h @ W_hi + b_i)\n",
    "            f_t = torch.sigmoid(x_t @ W_xf + h @ W_hf + b_f)\n",
    "            g_t = torch.tanh(x_t @ W_xc + h @ W_hc + b_c)\n",
    "            o_t = torch.sigmoid(x_t @ W_xo + h @ W_ho + b_o)\n",
    "            c_t = f_t * c + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            o[:,t,:] = h_t\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    state = (h, c)\n",
    "    return o, state\n",
    "  \n",
    "  def forward(self, inputs, state):\n",
    "    return self.lstm(inputs, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your output has the correct shape. You should see:\n",
    "\n",
    "```\n",
    "torch.Size([12, 8, 5])\n",
    "torch.Size([12, 5])\n",
    "torch.Size([12, 5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 8, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([12, 5])\n"
     ]
    }
   ],
   "source": [
    "test_lstm = LSTM(10, 5, device)\n",
    "test_src = torch.ones(12, 8, 10)\n",
    "test_src_len = torch.ones(12) * 8\n",
    "test_h = torch.zeros(12, 5).float()\n",
    "test_c = torch.zeros(12, 5).float()\n",
    "\n",
    "test_o, test_state = test_lstm((test_src, test_src_len), (test_h, test_c))\n",
    "\n",
    "print(test_o.shape)\n",
    "print(test_state[0].shape)\n",
    "print(test_state[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n",
    "</div>\n",
    "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Attention**\n",
    "\n",
    "    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n",
    "    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n",
    "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
    "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Masked Softmax**\n",
    "\n",
    "For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. We provide the implementation of this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_length):\n",
    "  \"\"\"\n",
    "  inputs:\n",
    "    X: 3-D tensor\n",
    "    valid_length: 1-D or 2-D tensor\n",
    "  \"\"\"\n",
    "  mask_value = -1e7 \n",
    "\n",
    "  X = X.to(device)\n",
    "  valid_length = valid_length.to(device)\n",
    "\n",
    "  if len(X.shape) == 2:\n",
    "    X = X.unsqueeze(1)\n",
    "\n",
    "  N, n, m = X.shape\n",
    "\n",
    "  if len(valid_length.shape) == 1:\n",
    "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
    "  else:\n",
    "    valid_length = valid_length.reshape((-1,))\n",
    "\n",
    "  mask = torch.arange(m)[None, :].to(device) >= valid_length[:, None]\n",
    "  X.view(-1, m)[mask] = mask_value\n",
    "\n",
    "  Y = torch.softmax(X, dim=-1)\n",
    "\n",
    "  \n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4667, 0.5333, 0.0000, 0.0000],\n",
       "         [0.5474, 0.4526, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2324, 0.5569, 0.2107, 0.0000],\n",
       "         [0.3379, 0.4132, 0.2489, 0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scaled Dot Product Attention**\n",
    "    - The scaled dot-product attention uses the score function as: $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
    "    </div>\n",
    "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
    "\n",
    "Implement the DotProductAttention below. Do not use any loops in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b7a6c1703c6c230a006a6b86326a3b9",
     "grade": false,
     "grade_id": "cell-eac4fccbcd4f068e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module): \n",
    "  def __init__(self):\n",
    "      super(DotProductAttention, self).__init__()\n",
    "\n",
    "  def forward(self, query, key, value, valid_length=None):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      query: tensor of size (B, n, d)\n",
    "      key: tensor of size (B, m, d)\n",
    "      value: tensor of size (B, m, dim_v)\n",
    "      valid_length: (B, )\n",
    "\n",
    "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
    "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
    "\n",
    "    Outputs:\n",
    "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the forward pass of DotProductAttention. Do not\n",
    "    # use any loops in your implementation.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    key = key.to(device)\n",
    "    key = key.transpose(-2,-1).float()\n",
    "    query = query.to(device)\n",
    "    \n",
    "    p = query@key\n",
    "    valid_length = valid_length.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "    soft = masked_softmax(p, valid_length).to(device)\n",
    "    value = value.to(device)\n",
    "\n",
    "    attention = soft@value\n",
    "\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Check for DotProductAttention\n",
    "\n",
    "Run the following snippet to check your implementation of DotProductAttention.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
    "\n",
    "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "        [[10., 11., 12., 13.]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = DotProductAttention()\n",
    "keys = torch.ones((2,10,2),dtype=torch.float)\n",
    "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
    "att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MLP Attention**\n",
    "\n",
    "    In MLP attention, we project both query and keys into $R^h$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n",
    "    \n",
    "    $$\n",
    "    \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})\n",
    "    $$\n",
    "    \n",
    "    where $\\mathbf{v}, \\mathbf{W_k}\\text{, and }\\mathbf{W_q}$ are learnable parameters.\n",
    "    \n",
    "Implement the MLP attention in matrix form without using any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9440f3519ad5f8037f192758aecca64a",
     "grade": false,
     "grade_id": "cell-6be727894d4fd817",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):  \n",
    "  def __init__(self, d_v, d_k, d_q):\n",
    "    super(MLPAttention, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_k: feature dimension of key\n",
    "      d_v: feature dimension of vector v\n",
    "      d_q: feature dimension of query\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize learnable parameters\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "\n",
    "    # self.W_k = (torch.randn(d_k)*0.1)\n",
    "    # self.W_q = (torch.randn(d_q)*0.1)\n",
    "    # self.v = (torch.randn(d_v)*0.1)\n",
    "\n",
    "    self.W_k = torch.nn.Linear(d_k, d_v, device=device)\n",
    "    self.W_q = torch.nn.Linear(d_q, d_v, device=device)\n",
    "    self.v = torch.nn.Linear(d_v, 1, device=device)\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "  def forward(self, query, key, value, valid_length):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      query: tensor of size (B, n, d)\n",
    "      key: tensor of size (B, m, d)\n",
    "      value: tensor of size (B, m, dim_v)\n",
    "      valid_length: either (B, )\n",
    "\n",
    "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
    "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
    "\n",
    "    Outputs:\n",
    "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the forward pass of MLPAttention. Do not\n",
    "    # use any loops in your implementation.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    key = key.to(device)\n",
    "    query = query.to(device)\n",
    "    valid_length = valid_length.to(device)\n",
    "    value = value.to(device)\n",
    "\n",
    "\n",
    "    key = self.W_k(key).unsqueeze(axis=1).to(device)\n",
    "    query = self.W_q(query).unsqueeze(axis=2).to(device)\n",
    "    scores = self.v(torch.tanh(key + query)).squeeze(axis=-1).to(device)\n",
    "    Y = torch.bmm(masked_softmax(scores, valid_length), value)\n",
    "    # END OF YOUR CODE\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Check for MLPAttention\n",
    "\n",
    "Run the following snippet to check your implementation of MLPAttention.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
    "\n",
    "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "        [[10., 11., 12., 13.]]], device='cuda:0', grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MLPAttention(4, 2, 2)\n",
    "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- **Using Attention in seq2seq Models**\n",
    "\n",
    "    <div>\n",
    "    <img src=\"https://d2l.ai/_images/seq2seq-attention.svg\" width=\"600\"/>\n",
    "    </div>\n",
    "Image source: https://d2l.ai/_images/seq2seq-attention.svg\n",
    "\n",
    "    Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n",
    "    \n",
    "    In rough pseudocode, this looks like:\n",
    "    ```\n",
    "    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n",
    "    decoder_input = concatenate([decoder_input, context])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Encoder-Decoder\n",
    "\n",
    "\n",
    "Build a seq2seq model with LSTM and attention.\n",
    "\n",
    "- Complete the Encoder forward() function.\n",
    "- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n",
    "- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbd2ff0f838eab4eeb305bd07bbd3a8e",
     "grade": false,
     "grade_id": "cell-85d8bda82bc92dd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
    "    super(Encoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of vallina RNN\n",
    "    \"\"\"\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = LSTM(embedding_dim, hidden_size, device)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, sources, valid_len):\n",
    "    ##############################################################################\n",
    "    # TODO: Implement LSTM Encoder forward pass\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "\n",
    "    # word_embedded = self.embedding(sources)\n",
    "    # N = word_embedded.shape[0]\n",
    "    \n",
    "    # h = sources.new_zeros(N, self.hidden_size).float() # initialize hidden state with zeros\n",
    "    \n",
    "    # o, h = self.enc(word_embedded, h)\n",
    "    # c=0\n",
    "    \n",
    "    # return o[np.arange(N), valid_len], (h, c)    \n",
    "\n",
    "    word_embedded = self.embedding(sources)\n",
    "\n",
    "    N = word_embedded.shape[0]\n",
    "    h = sources.new_zeros(N, self.hidden_size).float() \n",
    "    c = sources.new_zeros(N, self.hidden_size).float() \n",
    "    o, (h, c) = self.enc((word_embedded, 0), (h, c))\n",
    "    \n",
    "    # END OF YOUR CODE\n",
    "    return o, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8f15ad91df74611a4f70744a202ad53",
     "grade": false,
     "grade_id": "cell-154ce877082ed913",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
    "    super(Decoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of vallina RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
    "    self.att = DotProductAttention()\n",
    "    self.output_emb = nn.Linear(hidden_size, vocab_size, device=device)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, state, target, valid_len):\n",
    "    loss = 0\n",
    "    preds = []\n",
    "    valid_len = valid_len.to(device)\n",
    "    \n",
    "    ##############################################################################\n",
    "    # TODO: Implement LSTM Decoder forward pass. Your solution should also use\n",
    "    # self.att for attention.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code    \n",
    "\n",
    "    outputs, (h, c), src_len = state\n",
    "\n",
    "    T = target.size(1)\n",
    "\n",
    "    for t in range(T-1):\n",
    "      inputs = self.embedding(target[:,t].unsqueeze(1))\n",
    "      atten = self.att(h.unsqueeze(1), outputs, outputs, src_len)\n",
    "      inputs = torch.cat((inputs, atten), dim=-1)\n",
    "      o, (h, c) = self.enc((inputs, valid_len), (h, c))\n",
    "      out = self.output_emb(o.to(device)).float()\n",
    "      preds.append(out)\n",
    "      loss = loss + F.nll_loss(F.log_softmax(out[:,0]), target[:,t+1], ignore_index=valid_len[t+1])\n",
    "\n",
    "    preds = torch.cat(preds, dim=1).argmax(dim=-1)\n",
    "    # END OF YOUR CODE\n",
    "    return loss, preds\n",
    "  \n",
    "  def predict(self, state, target, valid_len):\n",
    "    pred = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement LSTM Encoder prediction. Your solution should also use\n",
    "    # self.att for attention.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code   \n",
    "\n",
    "    inputs = self.embedding(target[:, :1])\n",
    "    outputs, (h, c), src_len = state\n",
    "\n",
    "    preds = []\n",
    "    for t in range(MAX_LEN+1): # plus the 'eos' token\n",
    "      atten = self.att(h.unsqueeze(1), outputs, outputs, src_len)\n",
    "      # print(h.size())\n",
    "      # print(c.size())\n",
    "      inputs = torch.cat((inputs, atten), dim=-1)\n",
    "      o, (h, c) = self.enc((inputs, valid_len), (h, c))\n",
    "      out = self.output_emb(o.to(device)).float()\n",
    "      preds.append(out)\n",
    "      inputs = self.embedding(out.argmax(dim=-1))\n",
    "    \n",
    "    pred = torch.cat(preds, dim=1).argmax(dim=2)\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTLSTM(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
    "    super(NMTLSTM, self).__init__()\n",
    "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
    "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
    "    \n",
    "  def forward(self, src, src_len, tgt, tgt_len):\n",
    "    outputs, (h, c) = self.enc(src, src_len)\n",
    "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
    "    return loss, pred\n",
    "  \n",
    "  def predict(self, src, src_len, tgt, tgt_len):\n",
    "    outputs, (h, c) = self.enc(src, src_len)\n",
    "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bc6101f369326d21d0efe8439c652b",
     "grade": false,
     "grade_id": "cell-bfaaa623c7199b2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
      "iter 0 / 7800\tLoss:\t54.899147\n",
      "pred:\t tensor([195, 235, 293, 283, 362, 186, 255, 407, 407])\n",
      "\n",
      "tgt:\t tensor([ 14, 201, 146, 226,  11,   2,   0,   0,   0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 156 / 7800\tLoss:\t9.425118\n",
      "pred:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 312 / 7800\tLoss:\t7.231522\n",
      "pred:\t tensor([267, 341,   3,  24,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 92, 341, 139,  24,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 468 / 7800\tLoss:\t7.070657\n",
      "pred:\t tensor([ 3, 11, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([  3, 350,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 624 / 7800\tLoss:\t5.273014\n",
      "pred:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([15,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 780 / 7800\tLoss:\t5.712364\n",
      "pred:\t tensor([171,   3,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([321,   3,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 936 / 7800\tLoss:\t4.656765\n",
      "pred:\t tensor([14, 28,  3,  3, 11,  2,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14, 28, 75,  3, 11,  2,  0,  0,  0])\n",
      "\n",
      "iter 1092 / 7800\tLoss:\t4.553356\n",
      "pred:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 1248 / 7800\tLoss:\t3.790086\n",
      "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 1404 / 7800\tLoss:\t3.620337\n",
      "pred:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 1560 / 7800\tLoss:\t4.242450\n",
      "pred:\t tensor([14, 79, 28, 41,  3, 11,  2,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14, 79, 28, 41,  3, 11,  2,  0,  0])\n",
      "\n",
      "iter 1716 / 7800\tLoss:\t3.353245\n",
      "pred:\t tensor([307,   9, 139,   5,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([308,  37, 139,   5,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 1872 / 7800\tLoss:\t2.925863\n",
      "pred:\t tensor([ 14, 375,  41,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 14, 375,  41,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2028 / 7800\tLoss:\t2.835650\n",
      "pred:\t tensor([ 14, 116,  28,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 14, 116,  28,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2184 / 7800\tLoss:\t2.461363\n",
      "pred:\t tensor([ 48,  49, 113,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 48,  50, 113,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 2340 / 7800\tLoss:\t2.812308\n",
      "pred:\t tensor([ 48,   3,   3, 329,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 48,  72,   3, 329,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2496 / 7800\tLoss:\t2.459008\n",
      "pred:\t tensor([ 14, 203,  19,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([  3, 203,  19,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 2652 / 7800\tLoss:\t2.334226\n",
      "pred:\t tensor([3, 5, 5, 2, 0, 0, 0, 0, 0])\n",
      "\n",
      "tgt:\t tensor([3, 3, 5, 2, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 2808 / 7800\tLoss:\t2.440454\n",
      "pred:\t tensor([171, 261, 293,  84,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 92, 261, 293,  84,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 2964 / 7800\tLoss:\t2.354121\n",
      "pred:\t tensor([14, 11, 79,  3, 11,  2,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([374,  14,  79,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 3120 / 7800\tLoss:\t2.393387\n",
      "pred:\t tensor([ 14,  79,  96, 113,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 14, 375,  96, 113,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 3276 / 7800\tLoss:\t1.913054\n",
      "pred:\t tensor([ 14, 171, 120,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 14, 171, 120,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 3432 / 7800\tLoss:\t1.943292\n",
      "pred:\t tensor([14, 79, 29, 41,  3, 11,  2,  0,  0])\n",
      "\n",
      "tgt:\t tensor([ 14,  79, 277,  41, 353,  11,   2,   0,   0])\n",
      "\n",
      "iter 3588 / 7800\tLoss:\t2.236465\n",
      "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 3744 / 7800\tLoss:\t1.838758\n",
      "pred:\t tensor([ 14, 231,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 15, 231,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 3900 / 7800\tLoss:\t1.863658\n",
      "pred:\t tensor([14, 28,  9,  3, 11,  2,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14, 28,  9,  3, 11,  2,  0,  0,  0])\n",
      "\n",
      "iter 4056 / 7800\tLoss:\t1.937960\n",
      "pred:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 4212 / 7800\tLoss:\t2.002606\n",
      "pred:\t tensor([ 48,  50, 209,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 48,  50, 209,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 4368 / 7800\tLoss:\t1.400017\n",
      "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 4524 / 7800\tLoss:\t2.579388\n",
      "pred:\t tensor([155, 157,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([155, 157,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 4680 / 7800\tLoss:\t1.612499\n",
      "pred:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 4836 / 7800\tLoss:\t1.822516\n",
      "pred:\t tensor([267, 333,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([267, 118,  24,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 4992 / 7800\tLoss:\t2.032939\n",
      "pred:\t tensor([15,  3, 37,  3, 37,  2,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([ 14, 115,  37,   3,  11,   2,   0,   0,   0])\n",
      "\n",
      "iter 5148 / 7800\tLoss:\t1.389267\n",
      "pred:\t tensor([99,  9, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([99,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 5304 / 7800\tLoss:\t2.948864\n",
      "pred:\t tensor([199, 108,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([199,   3,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 5460 / 7800\tLoss:\t2.275380\n",
      "pred:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([15,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 5616 / 7800\tLoss:\t2.302059\n",
      "pred:\t tensor([ 14, 259, 325,  90,   3,  11,   2,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 15,  74, 325,  90,   3,  11,   2,   0,   0])\n",
      "\n",
      "iter 5772 / 7800\tLoss:\t1.844015\n",
      "pred:\t tensor([51,  9, 52, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([51,  9, 52, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 5928 / 7800\tLoss:\t1.939873\n",
      "pred:\t tensor([244, 320,  88,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([243, 320,  88,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 6084 / 7800\tLoss:\t1.617070\n",
      "pred:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 6240 / 7800\tLoss:\t1.931929\n",
      "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "iter 6396 / 7800\tLoss:\t1.566911\n",
      "pred:\t tensor([ 15, 119,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 15, 119,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 6552 / 7800\tLoss:\t1.739019\n",
      "pred:\t tensor([380, 265,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([  3, 265,  11,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "iter 6708 / 7800\tLoss:\t1.373968\n",
      "pred:\t tensor([ 92, 341, 383,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 92, 341, 382,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 6864 / 7800\tLoss:\t1.511421\n",
      "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
      "\n",
      "iter 7020 / 7800\tLoss:\t1.761662\n",
      "pred:\t tensor([ 9, 80, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "tgt:\t tensor([ 9, 80, 11,  2,  0,  0,  0,  0,  0])\n",
      "\n",
      "iter 7176 / 7800\tLoss:\t2.007849\n",
      "pred:\t tensor([ 36, 166,  88,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([ 36, 166,  88,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 7332 / 7800\tLoss:\t1.486319\n",
      "pred:\t tensor([100,   9, 192,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([100,   9, 192,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "iter 7488 / 7800\tLoss:\t1.525970\n",
      "pred:\t tensor([48,  3, 11,  3,  3, 11,  2,  0,  0])\n",
      "\n",
      "tgt:\t tensor([48,  3, 41, 72,  3, 11,  2,  0,  0])\n",
      "\n",
      "iter 7644 / 7800\tLoss:\t1.461521\n",
      "pred:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
      "\n",
      "tgt:\t tensor([171, 342, 235,  11,   2,   0,   0,   0,   0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_lstm(net, train_iter, lr, epochs, device):\n",
    "  # training\n",
    "  net = net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "  loss_list = []\n",
    "  print_interval = len(train_iter)\n",
    "  total_iter = epochs * len(train_iter)\n",
    "  for e in range(epochs):\n",
    "    net.train()\n",
    "    for i, train_data in enumerate(train_iter):\n",
    "      train_data = [ds.to(device) for ds in train_data]\n",
    "      \n",
    "      loss, pred = net(*train_data)\n",
    "\n",
    "      loss_list.append(loss.mean().cpu().detach())\n",
    "      optimizer.zero_grad()\n",
    "      loss.mean().backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      step = i + e * len(train_iter)\n",
    "      if step % print_interval == 0:\n",
    "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "        # print(pred)\n",
    "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
    "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
    "  return loss_list\n",
    "\n",
    "seed(1)\n",
    "batch_size = 32\n",
    "lr = 0.003\n",
    "##############################################################################\n",
    "# TODO: Find a good learning rate to train this model. Make sure your best\n",
    "# model is saved to the `lstm_net` variable.\n",
    "##############################################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "epochs = 50\n",
    "\n",
    "embedding_dim = 250\n",
    "hidden_size = 128\n",
    "\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
    "\n",
    "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Loss Curve\n",
    "\n",
    "Plot the loss curve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Curve of LSTM Attention')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n",
    "plt.title('Loss Curve of LSTM Attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your model. You should be able to get at least 68% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
      "pred:\t ['entrez', '!', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', \"t'en\", 'dois', 'une', '.', 'pad', 'pad', 'pad']\n",
      "\n",
      "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['a-t-il', 'raison', '?', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', 'suis', 'unk', '.', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['demande', 'à', 'unk', 'à', 'unk', 'à', 'unk', 'à', 'unk']\n",
      "\n",
      "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "Prediction Acc.: 0.9025\n"
     ]
    }
   ],
   "source": [
    "def comp_acc(pred, gt, valid_len):\n",
    "  N, T_gt = gt.shape[:2]\n",
    "  _, T_pr = pred.shape[:2]\n",
    "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
    "  len_mask = len_mask < valid_len[:, None]\n",
    "  \n",
    "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "  return pred_acc\n",
    "  \n",
    "def evaluate_lstm(net, train_iter, device):\n",
    "  acc_list = []\n",
    "  for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "    pred = net.predict(*train_data)\n",
    "\n",
    "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
    "    acc_list.append(pred_acc)\n",
    "    if i < 5:# print 5 samples from 5 batches\n",
    "      pred = pred[0].detach().cpu()\n",
    "      pred_seq = []\n",
    "      for t in range(MAX_LEN+1):\n",
    "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "        if pred_wd != 'eos':\n",
    "          pred_seq.append(pred_wd)\n",
    "\n",
    "      print('pred:\\t {}\\n'.format(pred_seq))\n",
    "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
    "\n",
    "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
    "  \n",
    "seed(1)\n",
    "batch_size = 32\n",
    "\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "evaluate_lstm(lstm_net, train_iter, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Recurrent Neural Networks can capture long-range, variable-length sequential information, but updating the current state relies on the previous states. Thus it cannot be parallelized across the entire sequence. In contrast, CNNs are easy to parallelize but they cannot capture sequential dependency within variable-length sequences and their receptive field is limited. Transformers resolve this dilemna by being able to capture long-range dependencies while also being easy to parallelize.\n",
    "\n",
    "Transformers consist of several different components and we will walk you through each of them. The original paper can be found [here](https://arxiv.org/pdf/1706.03762.pdf). [Here](http://jalammar.github.io/illustrated-transformer/) is a very informative blog about transformers that should also be a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Head Self-Attention**\n",
    "\n",
    "Multi-head self-attention is an extension of the dot-product attention we've previously implemented. The \"self-attention\" part means that the query, key, and value all come from the same sequence. For a sentence, this means that we are looking at how each word pays attention to other words in the same sentence. The \"multi-head\" part means instead of only having one attention map, we can have multiple. This means that for a given word in the sentence, it can pay attention to different parts of the sentence.\n",
    "\n",
    "The steps in the multi-head attention can be summarzied by the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. The multi-head self-attention takes the initial query $Q$, key $K$, and value $V$ as input. Note that, if not provided specifically, usually these are set to the same input embeddings $X=Q=K=V$ initially.\n",
    "   \n",
    "   1. Then, a linear projection is applied to $Q,K,V$ sepearately for each head $i=1,\\dots,h$. \n",
    "      $$\n",
    "   Q_i = QW^{Q}_i, K_i = KW^{K}_i, V_i = VW^{V}_i, i \\in [0, \\dots, h-1],\n",
    "   $$\n",
    "   \n",
    "   where $W^Q_i \\in \\mathcal{R}^{d_{model} \\times d_k}, W^K_i \\in \\mathcal{R}^{d_{model} \\times d_k}\\text{, and } W^V_i \\in \\mathcal{R}^{d_{model} \\times d_v}$.\n",
    "     \n",
    "   1. Apply the scaled dot-product attention to each of these projected set of queries, keys, and values:\n",
    "   $$\n",
    "   \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_iK^T_i}{\\sqrt{d_k}})V_i\n",
    "   $$\n",
    "   \n",
    "   1. Concatenate all the heads together and project it with another learned linear projections: \n",
    "   \n",
    "   $$\n",
    "   \\text{O} = \\text{Concate(head}_1, \\dots, \\text{head}_h) \\\\\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{O}W^o, \\hspace{10mm} \\text{where } W^o \\in \\mathcal{R}^{{hd_v} \\times d_{model}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good visualization from the above referenced [blog](http://jalammar.github.io/illustrated-transformer/) is shown below. Transformer stacks several multi-head attention modules together. For the first multi-head layer, the input is from the dataset, so an additional embedding layer is needed to project the input sequence into the appropriate dimensions. For subsequent layers, the output from the layer previous layer is directly used as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the MultiHeadAttention class below:\n",
    " - Complete the __init__() function, where the linear mappings for query, key, values, and output should be created.\n",
    " - Complete the forward() function, where the multi-head attention is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffb0d0e9e2f0f149afde973ef62dc33f",
     "grade": false,
     "grade_id": "cell-41f2921301bf3992",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, dk, num_heads,  **kwargs):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_model: int, the same d_model in paper, feature dimension of query/key/values\n",
    "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "      num_heads: int, number of heads used for this MultiHeadAttention\n",
    "    \"\"\"\n",
    "    self.num_heads = num_heads\n",
    "    self.attention = DotProductAttention()\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize the linear mappings for the query, key, and values.\n",
    "    # Also initialize the weight matrix for the output.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    self.W_q = nn.Linear(d_model, dk, device=device)\n",
    "    self.W_k = nn.Linear(d_model, dk, device=device)\n",
    "    self.W_v = nn.Linear(d_model, dk, device=device)\n",
    "    self.W_o = nn.Linear(dk*num_heads, d_model, device=device)\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "  def forward(self, query, key, value, valid_length):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      query: tensor of size (B, T, d_model)\n",
    "      key: tensor of size (B, T, d_model)\n",
    "      value: tensor of size (B, T, d_model)\n",
    "      valid_length: (B, )\n",
    "\n",
    "      B is the batch_size, T is length of sequence, d_model is the feature dimensions of query,\n",
    "      key, and value.\n",
    "\n",
    "    Outputs:\n",
    "      attention (B, T, d_model)\n",
    "      \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the forward pass of MultiHeadAttention.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "\n",
    "    B = query.size(0)\n",
    "    T = query.size(1)\n",
    "    d_model = query.size(2)\n",
    "\n",
    "    query, key, value = query.to(device).repeat(self.num_heads, 1, 1).reshape(self.num_heads, B, T, d_model), key.to(device).repeat(self.num_heads, 1, 1).reshape(self.num_heads, B, T, d_model), value.to(device).repeat(self.num_heads, 1, 1).reshape(self.num_heads, B, T, d_model)\n",
    "    q, k, v = self.W_q(query), self.W_k(key), self.W_v(value)\n",
    "\n",
    "    z = torch.zeros((9, 2, 4, 90), device=device)\n",
    "\n",
    "    last = z.size(0) * z.size(3)\n",
    "\n",
    "    for i in range(9):\n",
    "      attention = self.attention(q[i], k[i], v[i], valid_length)\n",
    "      z[i] = attention\n",
    "\n",
    "    attention = self.W_o(torch.moveaxis(z, 0, 3).reshape(B, T, last))\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = MultiHeadAttention(5, 90, 9)\n",
    "X = torch.ones((2, 4, 5), device=device)\n",
    "valid_len = torch.tensor([2, 3], device=device)\n",
    "cell(X, X, X, valid_len).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network\n",
    "\n",
    "Another key component in the Transformer block is the position-wise feed-forward network (FFN). It's called position-wise FFN because the linear mapping is applied to each position separately and identically. For example, for an embedded input of size $N \\times T \\times D_{in}$, there are $N*T$ vectors of dimension $D_{in}$. If we apply a one layer position-wise FFN with weights of size $D_{in} \\times D_{out}$. The linear projection will be applied to each of the $N*T$ vectors separately and identically. Thus, the output would have size $N \\times T \\times D_{out}$. Another way to think about this is that this is the same as a 1x1 convolution mapping from $D_{in}$ channels to $D_{out}$ channels.\n",
    "\n",
    "Transformers stack two layers of position-wise FFN together, with a ReLU activation in between:\n",
    "\n",
    "$$\n",
    "\\text{PositionWiseFFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "Complete the class PositionWiseFFN:\n",
    "\n",
    "- Complete the __init__() function, where two position-wise FFN should be created.\n",
    "- Complete the forward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "144103b4a6258c267c6a327e33851042",
     "grade": false,
     "grade_id": "cell-39ca6d33ccf1a8f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "  def __init__(self, input_size, ffn_l1_size, ffn_l2_size):\n",
    "    super(PositionWiseFFN, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      input_size: int, feature dimension of the input\n",
    "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize the feed forward network for PositionWiseFFN\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    self.L1 = nn.Linear(input_size, ffn_l1_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.L2 = nn.Linear(input_size, ffn_l2_size)\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      X: tensor of size (N, T, D_in)\n",
    "    Output:\n",
    "      o: tensor of size (N, T, D_out)\n",
    "    \"\"\"\n",
    "    o = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass of PositionWiseFFN\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    o = self.L2(self.relu(self.L1(X)))\n",
    "    # END OF YOUR CODE\n",
    "    return o\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your result. Expected output\n",
    "\n",
    "```\n",
    "[[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
    "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
    "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
       "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
       "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(1)\n",
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Replacing RNNs with the multi-head attention layer and applying the position-wise feed-forward network makes the computation parallelizable since these modules compute the output of each item in the sequence independently. However, since every item is processed in parallel, there is no notion of ordering of the sequence. For an input sentence, this means that the transformer doesn't know the ordering of the words in the sentence. For most tasks, this ordering is very important. To address this, transformers propose adding a positional encoding to each input that corresponds to the position in the sequence. This means that we take the position of each word in the sentence (eg. 0, 1, 2, etc...) and map it to some $d_{model}$-dimensional embedding. We then add this embedding with every input item so that the input is not position-aware. Transformers use the following sinusoidal positional encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE_{(pos, 2i)} &= sin(pos / 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= cos(pos / 10000^{2i/d_{model}}) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "An example borrowed from this [blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) can give an ituition how this positional encoding works. Suppose you want to encode the number from $0$ to $8$ using binary encoding, the result would like this:\n",
    "$$\n",
    "\\begin{align*}\n",
    "0: && 0  0  0 \\\\\n",
    "1: && 0  0  1 \\\\\n",
    "2: && 0  1  0 \\\\\n",
    "3: && 0  1  1 \\\\\n",
    "4: && 1  0  0 \\\\\n",
    "5: && 1  0  1 \\\\\n",
    "6: && 1  1  0 \\\\\n",
    "7: && 1  1  1 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Note the frequency of ones in each digit is different. Thus, words at different locations will have different embedding features (digits in the example). The figure below visualized a position encoding matrix of dimension $\\mathcal{R}^{50 \\times 128}$\n",
    "\n",
    "The forward pass of the positional encoding should add the positional embedding to the input:\n",
    "\n",
    "`y = x + positional_encoding(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the class PositionalEncoding:\n",
    "- Complete the __init__() function, where the tensor $PE$ should be created.\n",
    "- Complete the forward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f724034b91387df0b62a10fb7268858",
     "grade": false,
     "grade_id": "cell-fc6933701b990269",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, dim, device, max_len=1000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      dim: feature dimension of the positional encoding\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize positional encoding. You should create `self.pe`\n",
    "    # here according to the definition above. The positional encoding should\n",
    "    # support up to position `max_len`.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    self.pe = torch.zeros(max_len, dim, device=device)\n",
    "    for pos in range(max_len):\n",
    "      for i in range(0, dim, 2):\n",
    "        self.pe[pos, i] = math.sin(pos / 10000**(2*i/dim))\n",
    "      for i in range(1, dim, 2):\n",
    "        self.pe[pos, i] = math.cos(pos / 10000**(2*i/dim))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "\n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tensor of size (N, T, D_in)\n",
    "    Output:\n",
    "      Y: tensor of the same size of X\n",
    "    \"\"\"\n",
    "    Y = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for positional encoding. After getting the positional\n",
    "    # encoding with regards to the time dimension, add it to the input X.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    # print(X.size())\n",
    "    # print(self.pe.size())\n",
    "    Y = X\n",
    "    for i in range(X.size(0)):\n",
    "      Y[i] += self.pe[0:X.size(1), 0:X.size(2)]\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your result. Expected output\n",
    "\n",
    "```\n",
    "[[1., 2., 1., 2., 1., 2., 1., 2., 1., 2.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000,\n",
       "         2.0000],\n",
       "        [1.8415, 1.9875, 1.0251, 2.0000, 1.0006, 2.0000, 1.0000, 2.0000, 1.0000,\n",
       "         2.0000],\n",
       "        [1.9093, 1.9502, 1.0502, 2.0000, 1.0013, 2.0000, 1.0000, 2.0000, 1.0000,\n",
       "         2.0000],\n",
       "        [1.1411, 1.8891, 1.0753, 1.9999, 1.0019, 2.0000, 1.0000, 2.0000, 1.0000,\n",
       "         2.0000],\n",
       "        [0.2432, 1.8057, 1.1003, 1.9999, 1.0025, 2.0000, 1.0001, 2.0000, 1.0000,\n",
       "         2.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(1)\n",
    "pe = PositionalEncoding(10, device)\n",
    "pe(torch.ones((2, 5, 10), device=device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and Norm\n",
    "\n",
    "Transformers use a residual connection followed by a layer normalization layer to connect the inputs and outputs of other layers. To be specific, an \"add and norm\" layer is appended after each multi-head attention layer and the position-wise FFN layer. *The code for AddNorm Layer is given as below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, dropout, embedding_size):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "\n",
    "The following figure gives a simple example of how the Transformer is built on these components introduced above. It's easy to see that the encoder of the Transformer consists of several identical encoder blocks, and so does the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the forward() function for the EncoderBlock and DecoderBlock. Note that, for the decoder, when applying self-attention, the sequential queries **cannot** attend to those at later time steps. For example, in a sequence, the query entry at time step 5 can only observe the first 5 entries. You can use `valid_length` to enforce this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a74c7336c9f0f3646b41cd064e1ebadf",
     "grade": false,
     "grade_id": "cell-544ed4c7777ee925",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads, dropout):\n",
    "    super(EncoderBlock, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_model: int, feature dimension of query/key/value\n",
    "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "      num_heads: int, number of head for multi-head attention layer.\n",
    "      dropout: dropout probability for dropout layer.\n",
    "      \n",
    "    \"\"\"\n",
    "    self.attention = MultiHeadAttention(d_model, d_k, num_heads)\n",
    "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
    "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
    "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
    "\n",
    "  def forward(self, X, valid_length):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tensor of size (N, T, D), embedded input sequences\n",
    "      valid_length: tensor of size (N), valid lengths for each sequence\n",
    "    \"\"\"\n",
    "    Y = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for the EncoderBlock. Use the figure above\n",
    "    # for guidance:\n",
    "    # attention -> add+norm -> feed forward -> add+norm\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    Y = self.addnorm_2(self.ffn(self.addnorm_1(self.attention(X, X, X, valid_length))))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77dbfb80f87e8bc1beb98efaa7c01e61",
     "grade": false,
     "grade_id": "cell-8d2fafea68ccbc9f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads,\n",
    "             dropout, **kwargs):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_model: int, feature dimension of query/key/value\n",
    "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "      num_heads: int, number of head for multi-head attention layer.\n",
    "      dropout: dropout probability for dropout layer.\n",
    "      \n",
    "    \"\"\"\n",
    "    self.attention_1 = MultiHeadAttention(d_model, d_k, num_heads)\n",
    "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
    "    self.attention_2 = MultiHeadAttention(d_model, d_model, num_heads)\n",
    "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
    "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
    "    self.addnorm_3 = AddNorm(dropout, d_model)\n",
    "\n",
    "  def forward(self, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tensor of size (N, T, D), embedded input sequences\n",
    "      **kwargs: other arguments you think is necessary for implementation\n",
    "    Outputs:\n",
    "      Y: tensor of size (N, T, D_out)\n",
    "      \n",
    "      Feel free to output variables if necessary.\n",
    "    \"\"\"\n",
    "    Y = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for the DecoderBlock. Use the figure above\n",
    "    # for guidance:\n",
    "    # self attention -> add+norm -> enc-dec attention -> add+norm -> feed forward -> add+norm\n",
    "    # for the first attention layer, make sure to construct a `valid_length` that\n",
    "    # ensures each element cannot attend to later elements in the sequence.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    valid_len = torch.tensor([2, 3], device=device)\n",
    "    Y = self.addnorm_3(self.ffn(self.addnorm_2(self.attention_2(self.addnorm_1(self.attention_1(X, valid_len))))))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer  Implementation\n",
    "\n",
    "By stacking two encoder blocks and two decoder blocks, build the Transformer using the above components. \n",
    "\n",
    "- Implement the Encoder of Transformer:\n",
    " - Complete the __init__() function with a word embedding layer and several EncoderBlocks.\n",
    " - Complete the forward() function\n",
    "- Implement the Decoder of Transformer\n",
    " - Complete the __init__() function\n",
    " - Complete the forward() function\n",
    "- Implement the Transformer\n",
    " - Complete the forward() function\n",
    " - Complete the predict() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae68e2f2cb0af15282eeaa8ff420a15c",
     "grade": false,
     "grade_id": "cell-33857dd68f665a2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
    "               num_heads, num_layers, dropout, device):\n",
    "    super(TransformerEncoder, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_model: int, feature dimension of query/key/value\n",
    "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "      num_heads: int, number of head for multi-head attention layer.\n",
    "      dropout: dropout probability for dropout layer.\n",
    "      num_layers: number of encoder blocks\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement init() function for TransformerEncoder. See forward() notes\n",
    "    # for more details.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    self.d_model = d_model\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model,device)\n",
    "    (self,d_model, device)\n",
    "    self.blks = nn.Sequential()\n",
    "    for i in range(num_layers):\n",
    "        self.blks.add_module(\"block\"+str(i),\n",
    "            EncoderBlock(d_model, d_model, ffn_l1_size, ffn_l2_size, num_heads, dropout))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "  def forward(self, X, valid_length):\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for the TransformerEncoder\n",
    "    # First, use an embedding so each element in X is d_model (hint: use nn.Embedding)\n",
    "    # Then, apply the positional embedding to each element\n",
    "    # Lastly, pass the resulting input into num_layers of EncoderBlocks\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    X = self.pos_encoding(self.embedding(X) * math.sqrt(self.d_model))\n",
    "    self.attention_weights = [None] * len(self.blks)\n",
    "    for i, blk in enumerate(self.blks):\n",
    "        X = blk(X, valid_length)\n",
    "        self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "765b53867f3968948b51990b9c69f80f",
     "grade": false,
     "grade_id": "cell-eefed3b5de8d2f3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
    "             num_heads, num_layers, dropout, device):\n",
    "    super(TransformerDecoder, self).__init__()\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      d_model: int, feature dimension of query/key/value\n",
    "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "      num_heads: int, number of head for multi-head attention layer.\n",
    "      dropout: dropout probability for dropout layer.\n",
    "      num_layers: number of decoder blocks\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement init() function for TransformerDecoder\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, device)\n",
    "    self.blks = nn.Sequential()\n",
    "    for i in range(num_layers):\n",
    "        self.blks.add_module(\"block\"+str(i),\n",
    "        DecoderBlock(d_model, d_model, ffn_l1_size, ffn_l2_size, num_heads,dropout))\n",
    "    self.dense = nn.Linear(d_model, vocab_size)\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "\n",
    "  def forward(self, X, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tensor of size (N, T, D), embedded input sequences\n",
    "      valid_length: tensor of size (N,), valid lengths for each sequence\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for the TransformerDecoder. This will look\n",
    "    # very similar to the TransformerEncoder.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    X = self.pos_encoding(self.embedding(X) * math.sqrt(self.d_model))\n",
    "    self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "    for i, blk in enumerate(self.blks):\n",
    "        X, state = blk(X, state)\n",
    "        self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "        self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "    # END OF YOUR CODE\n",
    "    return self.dense(X), X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68446581beb8f1b29dcb65023d32bcdf",
     "grade": false,
     "grade_id": "cell-1a06f1a890069bc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "  def __init__(self, encoder, decoder, **kwargs):\n",
    "    super(Transformer, self).__init__(**kwargs)\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
    "    \"\"\"Forward function\"\"\"\n",
    "    loss = 0\n",
    "    pred = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass of transformer\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    encOut = self.encoder(src_array,src_valid_len)\n",
    "    decOut = self.decoder(tgt_array,encOut, tgt_valid_len)\n",
    "    preds = nn.Linear(decOut)\n",
    "    loss = masked_softmax(preds, tgt_valid_len)\n",
    "    # END OF YOUR CODE\n",
    "    return loss, pred\n",
    "        \n",
    "  def predict(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
    "    pred = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement predict() of transformer\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    l,pred = self.forward()\n",
    "    # END OF YOUR CODE\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a good learning rate for training this model. Feel free to tune other hyperparameters as well as long as your best model is saved in `transformer_net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4caf1154ac2fe22f1709fdf9e258be88",
     "grade": false,
     "grade_id": "cell-535ea1c455d3fe85",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (90) must match the existing size (512) at non-singleton dimension 2.  Target sizes: [2, 4, 90].  Tensor sizes: [32, 10, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/3694459613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mtransformer_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mtransformer_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/3694459613.py\u001b[0m in \u001b[0;36mtrain_transformer\u001b[1;34m(net, train_iter, lr, epochs, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/591409492.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src_array, src_valid_len, tgt_array, tgt_valid_len)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m##############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Replace \"pass\" statement with your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mencOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msrc_valid_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdecOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_valid_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/3377845064.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, valid_length)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# END OF YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/3586383023.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, valid_length)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m##############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Replace \"pass\" statement with your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddnorm_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddnorm_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;31m# END OF YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32168/505215399.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, valid_length)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m       \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m       \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_o\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (90) must match the existing size (512) at non-singleton dimension 2.  Target sizes: [2, 4, 90].  Tensor sizes: [32, 10, 512]"
     ]
    }
   ],
   "source": [
    "def train_transformer(net, train_iter, lr, epochs, device):\n",
    "  # training\n",
    "  net = net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "  loss_list = []\n",
    "  print_interval = len(train_iter)\n",
    "  total_iter = epochs * len(train_iter)\n",
    "  for e in range(epochs):\n",
    "    net.train()\n",
    "    for i, train_data in enumerate(train_iter):\n",
    "      train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "      loss, pred = net(*train_data)\n",
    "\n",
    "      loss_list.append(loss.mean().detach())\n",
    "      optimizer.zero_grad()\n",
    "      loss.mean().backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      step = i + e * len(train_iter)\n",
    "      if step % print_interval == 0:\n",
    "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
    "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
    "  return loss_list\n",
    "\n",
    "\n",
    "# hyper-params: feel free to modify the values and numbers of hyper-params \n",
    "\n",
    "# training\n",
    "seed(1)\n",
    "batch_size = 32\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "\n",
    "embedding_dim = 250\n",
    "hidden_size = 128\n",
    "\n",
    "#transformer hp\n",
    "d_model = 512\n",
    "ffn_l1_size = 2048\n",
    "ffn_l2_size = d_model\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "lr = 0.003\n",
    "##############################################################################\n",
    "# TODO: Find a good learning rate to train this model. Make sure your best\n",
    "# model is saved to the `transformer_net` variable. Feel free to tune other hyperparameters\n",
    "# as well.\n",
    "##############################################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "epochs = 50\n",
    "\n",
    "device = torch.device('cuda:0') # cuda:0 if you have gpu\n",
    "\n",
    "encoder = TransformerEncoder(vocab_eng.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
    "                             num_heads, num_layers, dropout, device=device)\n",
    "decoder = TransformerDecoder(vocab_fra.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
    "                             num_heads, num_layers, dropout, device=device)\n",
    "transformer_net = Transformer(encoder, decoder)\n",
    "\n",
    "transformer_loss_list = train_transformer(transformer_net, train_iter, lr, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(transformer_loss_list)), transformer_loss_list)\n",
    "plt.title('Loss Curve of Transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your model. You should be able to get at least 73% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_acc(pred, gt, valid_len):\n",
    "  N, T_gt = gt.shape[:2]\n",
    "  _, T_pr = pred.shape[:2]\n",
    "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
    "  len_mask = len_mask < valid_len[:, None]\n",
    "  \n",
    "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "  return pred_acc\n",
    "  \n",
    "def evaluate_transformer(net, train_iter, device):\n",
    "  net.eval()\n",
    "  acc_list = []\n",
    "  for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "    pred = net.predict(*train_data)\n",
    "\n",
    "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
    "    acc_list.append(pred_acc)\n",
    "    if i < 5:# print 5 samples from 5 batches\n",
    "      pred = pred[0].detach().cpu()\n",
    "      pred_seq = []\n",
    "      for t in range(MAX_LEN+1):\n",
    "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "        if pred_wd != 'eos':\n",
    "          pred_seq.append(pred_wd)\n",
    "\n",
    "      print('pred:\\t {}\\n'.format(pred_seq))\n",
    "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
    "\n",
    "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
    "  \n",
    "seed(1)\n",
    "batch_size = 32\n",
    "\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "\n",
    "evaluate_transformer(transformer_net, train_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
